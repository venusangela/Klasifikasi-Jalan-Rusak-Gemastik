{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xml.etree.ElementTree as ET\n",
    "\n",
    "# def parse_annotation(xml_path):\n",
    "#     f = open(xml_path, \"r\")\n",
    "#     xml_string = f.read()\n",
    "#     root = ET.fromstring(xml_string)\n",
    "    \n",
    "#     annotation = {}\n",
    "#     annotation['folder'] = root.find('folder').text\n",
    "#     annotation['filename'] = root.find('filename').text\n",
    "    \n",
    "#     size = root.find('size')\n",
    "#     annotation['width'] = int(size.find('width').text)\n",
    "#     annotation['height'] = int(size.find('height').text)\n",
    "#     try:\n",
    "#         objects = root.findall('object')\n",
    "#         print(objects)\n",
    "#         annotation['objects'] = []\n",
    "        \n",
    "#         for object_element in objects:\n",
    "#             obj = {}\n",
    "#             obj['name'] = object_element.find('name').text\n",
    "            \n",
    "#             bndbox = object_element.find('bndbox')\n",
    "#             obj['bndbox'] = {}\n",
    "#             obj['bndbox']['xmin'] = int(bndbox.find('xmin').text)\n",
    "#             obj['bndbox']['ymin'] = int(bndbox.find('ymin').text)\n",
    "#             obj['bndbox']['xmax'] = int(bndbox.find('xmax').text)\n",
    "#             obj['bndbox']['ymax'] = int(bndbox.find('ymax').text)\n",
    "            \n",
    "#             annotation['objects'].append(obj)\n",
    "#     except:\n",
    "#         print(xml_path)\n",
    "\n",
    "#     # parsed_data = parse_annotation(xml_string)\n",
    "#     return annotation\n",
    "# # print(parsed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_label(parsed_data):\n",
    "#     try:\n",
    "#         obj_name = [dict['name'] for dict in parsed_data['objects']]\n",
    "#     except:\n",
    "#         obj_name = []\n",
    "#     return obj_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "code = []\n",
    "for list_rdd in os.listdir(\"RDD2022\"):\n",
    "    for annot in os.listdir(f\"RDD2022/{list_rdd}/train/annotations/xmls\"):\n",
    "        data = parse_annotation(os.path.join(f\"RDD2022/{list_rdd}/train/annotations/xmls\", annot))\n",
    "        get_l = get_label(data)\n",
    "        code.extend(get_l)\n",
    "\n",
    "df = pd.DataFrame(code)\n",
    "df[0].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obj_code(obj_name):\n",
    "    if obj_name == \"D00\":\n",
    "        return 0\n",
    "    elif obj_name == \"D01\":\n",
    "        return 1\n",
    "    elif obj_name == \"D10\":\n",
    "        return 2\n",
    "    elif obj_name == \"D11\":\n",
    "        return 3\n",
    "    elif obj_name == \"D20\":\n",
    "        return 4\n",
    "    elif obj_name == \"D40\":\n",
    "        return 5\n",
    "    elif obj_name == \"D43\":\n",
    "        return 6\n",
    "    elif obj_name == \"D44\":\n",
    "        return 7\n",
    "    elif obj_name == \"D50\":\n",
    "        return 8 # manhole cover\n",
    "    elif obj_name == \"Block crack\":\n",
    "        return 9\n",
    "    elif obj_name == \"Repair\":\n",
    "        return 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code(parsed_data):\n",
    "    try:\n",
    "        # obj_name = [get_obj_code(dict['name']) for dict in parsed_data['objects']]\n",
    "        obj_name = [0 for dict in parsed_data['objects']]\n",
    "    except:\n",
    "        obj_name = []\n",
    "    return obj_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xml_path = './RDD2022/Norway/train/annotations/xmls/Norway_007037.xml'\n",
    "# parsed_data = parse_annotation(xml_path)\n",
    "# print(parsed_data)\n",
    "# # obj_name = [0 for dict in parsed_data['objects'] if len(parsed_data['objects']) > 0]\n",
    "# # print(obj_name)\n",
    "# if len(parsed_data['objects']) > 0:\n",
    "#     print(\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_labels(label_path, parsed_data):\n",
    "    if len(parsed_data['objects']) > 0:\n",
    "        try:\n",
    "            obj_name = [0 for dict in parsed_data['objects']]\n",
    "            img_width = parsed_data['width']\n",
    "            img_height = parsed_data['height']\n",
    "            x_center = [((dict['bndbox']['xmax'] + dict['bndbox']['xmin'])/2)/img_width for dict in parsed_data['objects']]\n",
    "            y_center = [((dict['bndbox']['ymax'] + dict['bndbox']['ymin'])/2)/img_height for dict in parsed_data['objects']]\n",
    "            width = [(dict['bndbox']['xmax'] - dict['bndbox']['xmin'])/img_width for dict in parsed_data['objects']]\n",
    "            height = [(dict['bndbox']['ymax'] - dict['bndbox']['ymin'])/img_height for dict in parsed_data['objects']]\n",
    "\n",
    "            file = open(label_path, \"a\")\n",
    "            for i in range(len(parsed_data['objects'])):\n",
    "                if i == 0:\n",
    "                    file.write(f\"{obj_name[i]} {x_center[i]} {y_center[i]} {width[i]} {height[i]}\")\n",
    "                else:\n",
    "                    file.write(f\"\\n{obj_name[i]} {x_center[i]} {y_center[i]} {width[i]} {height[i]}\") \n",
    "        except:\n",
    "            # file = open(label_path, \"a\")\n",
    "            pass\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def parse_annotation(xml_path):\n",
    "    f = open(xml_path, \"r\")\n",
    "    xml_string = f.read()\n",
    "    root = ET.fromstring(xml_string)\n",
    "    \n",
    "    annotation = {}\n",
    "    annotation['folder'] = root.find('folder').text\n",
    "    annotation['filename'] = root.find('filename').text\n",
    "    \n",
    "    size = root.find('size')\n",
    "    annotation['width'] = int(size.find('width').text)\n",
    "    annotation['height'] = int(size.find('height').text)\n",
    "    \n",
    "    objects = root.findall('object')\n",
    "    # print(f\"Number of objects: {len(objects)}\")\n",
    "    annotation['objects'] = []\n",
    "        \n",
    "    for object_element in objects:\n",
    "        obj = {}\n",
    "        obj['name'] = object_element.find('name').text\n",
    "            \n",
    "        bndbox = object_element.find('bndbox')\n",
    "        obj['bndbox'] = {}\n",
    "        obj['bndbox']['xmin'] = float(bndbox.find('xmin').text)\n",
    "        obj['bndbox']['ymin'] = float(bndbox.find('ymin').text)\n",
    "        obj['bndbox']['xmax'] = float(bndbox.find('xmax').text)\n",
    "        obj['bndbox']['ymax'] = float(bndbox.find('ymax').text)\n",
    "            \n",
    "        annotation['objects'].append(obj)\n",
    "\n",
    "    return annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obj_code(obj_name):\n",
    "    if obj_name == \"D00\" or obj_name == \"D10\" or obj_name == \"D20\" or obj_name == \"D40\":\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_labels(label_path, parsed_data):\n",
    "    if len(parsed_data['objects']) > 0:\n",
    "        try:\n",
    "            obj_name = []\n",
    "            img_width = parsed_data['width']\n",
    "            img_height = parsed_data['height']\n",
    "            x_center = []\n",
    "            y_center = []\n",
    "            width = []\n",
    "            height = []\n",
    "\n",
    "            for dict in parsed_data['objects']:\n",
    "                if dict['name'] in ['D00', 'D10', 'D20', 'D40']:\n",
    "                    obj_name.append(0)\n",
    "                    x_center.append(((dict['bndbox']['xmax'] + dict['bndbox']['xmin'])/2)/img_width)\n",
    "                    y_center.append(((dict['bndbox']['ymax'] + dict['bndbox']['ymin'])/2)/img_height)\n",
    "                    width.append((dict['bndbox']['xmax'] - dict['bndbox']['xmin'])/img_width)\n",
    "                    height.append((dict['bndbox']['ymax'] - dict['bndbox']['ymin'])/img_height)\n",
    "\n",
    "            file = open(label_path, \"a\")\n",
    "            for i in range(len(obj_name)):\n",
    "                if i == 0:\n",
    "                    file.write(f\"{obj_name[i]} {x_center[i]} {y_center[i]} {width[i]} {height[i]}\")\n",
    "                else:\n",
    "                    file.write(f\"\\n{obj_name[i]} {x_center[i]} {y_center[i]} {width[i]} {height[i]}\") \n",
    "        except:\n",
    "            # file = open(label_path, \"a\")\n",
    "            pass\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "if os.path.exists(\"data\"):\n",
    "    shutil.rmtree(\"data\")\n",
    "os.mkdir(\"data\")\n",
    "os.mkdir(\"data/images\")\n",
    "os.mkdir(\"data/labels\")\n",
    "os.mkdir(\"data/train\")\n",
    "# os.mkdir(\"data/test\")\n",
    "os.mkdir(\"data/val\")\n",
    "os.mkdir(\"data/train/images\")\n",
    "# os.mkdir(\"data/test/images\")\n",
    "os.mkdir(\"data/val/images\")\n",
    "os.mkdir(\"data/train/labels\")\n",
    "# os.mkdir(\"data/test/labels\")\n",
    "os.mkdir(\"data/val/labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "China_Drone begin\n",
      "China_Drone end\n",
      "\n",
      "China_MotorBike begin\n",
      "China_MotorBike end\n",
      "\n",
      "Czech begin\n",
      "Czech end\n",
      "\n",
      "India begin\n",
      "India end\n",
      "\n",
      "Japan begin\n",
      "Japan end\n",
      "\n",
      "Norway begin\n",
      "Norway end\n",
      "\n",
      "United_States begin\n",
      "United_States end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for list_rdd in os.listdir(\"RDD2022\"):\n",
    "    print(f\"{list_rdd} begin\")\n",
    "    for annot in os.listdir(f\"RDD2022/{list_rdd}/train/annotations/xmls\"):\n",
    "        data = parse_annotation(os.path.join(f\"RDD2022/{list_rdd}/train/annotations/xmls\", annot))\n",
    "        txt_path = annot.split(\".\")[0]\n",
    "        write_labels(f\"data/labels/{txt_path}.txt\", data)\n",
    "        # shutil.copy(src=f\"RDD2022/{list_rdd}/train/images/{txt_path}.jpg\", dst=f\"data/images/{txt_path}.jpg\")\n",
    "    print(f\"{list_rdd} end\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26660\n",
      "38384\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(\"./data/labels\")))\n",
    "print(len(os.listdir(\"./data/images\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26660"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(\"./data/labels\"))\n",
    "len(os.listdir(\"./data/images\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11724"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_damage = []\n",
    "data_dir = \"./data/images\"\n",
    "labels = os.listdir(\"./data/labels\")\n",
    "paths = [i.split(\".\")[0] for i in labels]\n",
    "\n",
    "for image in os.listdir(data_dir):\n",
    "    if image.split(\".\")[0] not in paths:\n",
    "        no_damage.append(image)\n",
    "len(no_damage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in no_damage:\n",
    "    os.remove(os.path.join(data_dir, image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26660"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(\"./data/images\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"./data/images\"\n",
    "label_dir = \"./data/labels\"\n",
    "\n",
    "img_paths = []\n",
    "label_paths = []\n",
    "for img in os.listdir(img_dir):\n",
    "    img_paths.append(os.path.join(img_dir, img))\n",
    "\n",
    "for label in os.listdir(label_dir):\n",
    "    label_paths.append(os.path.join(label_dir, label))\n",
    "\n",
    "data_path = pd.DataFrame()\n",
    "data_path['img_path'] = img_paths\n",
    "data_path['label_path'] = label_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val = train_test_split(data_path, test_size=0.2, random_state=0)\n",
    "# X_train, X_val = train_test_split(X_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path.to_csv(\"data_path.csv\")\n",
    "X_train.to_csv(\"X_train.csv\")\n",
    "# X_test.to_csv(\"X_test.csv\")\n",
    "X_val.to_csv(\"X_val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in X_train['img_path']:\n",
    "    img_p = img.split(\"\\\\\")[1]\n",
    "    new_img = f\".data/train/images/{img_p}\"\n",
    "    shutil.move(src=img,dst=new_img)\n",
    "\n",
    "for label in X_train['label_path']:\n",
    "    label_p = label.split(\"\\\\\")[1]\n",
    "    new_label = f\"./data/train/labels/{label_p}\"\n",
    "    shutil.move(src=label,dst=new_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in X_val['img_path']:\n",
    "    img_p = img.split(\"\\\\\")[1]\n",
    "    new_img = f\"./data/val/images/{img_p}\"\n",
    "    shutil.move(src=img,dst=new_img)\n",
    "\n",
    "for label in X_val['label_path']:\n",
    "    label_p = label.split(\"\\\\\")[1]\n",
    "    new_label = f\"./data/val/labels/{label_p}\"\n",
    "    shutil.move(src=label,dst=new_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for img in X_test['img_path']:\n",
    "#     img_p = img.split(\"\\\\\")[1]\n",
    "#     new_img = f\"./data/test/images/{img_p}\"\n",
    "#     shutil.move(src=img,dst=new_img)\n",
    "\n",
    "# for label in X_test['label_path']:\n",
    "#     label_p = label.split(\"\\\\\")[1]\n",
    "#     new_label = f\"./data/test/labels/{label_p}\"\n",
    "#     shutil.move(src=label,dst=new_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the image\n",
    "image_path = './data/train/images/Norway_007650.jpg'\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Read the annotation file\n",
    "annotation_file = './data/train/labels/Norway_007650.txt'\n",
    "with open(annotation_file, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Draw bounding boxes\n",
    "height, width, _ = image.shape\n",
    "for line in lines:\n",
    "    class_label, x_norm, y_norm, width_norm, height_norm = line.split()\n",
    "    x_norm, y_norm, width_norm, height_norm = map(float, (x_norm, y_norm, width_norm, height_norm))\n",
    "\n",
    "    # Convert normalized coordinates to absolute coordinates\n",
    "    x = int(x_norm * width)\n",
    "    y = int(y_norm * height)\n",
    "    box_width = int(width_norm * width)\n",
    "    box_height = int(height_norm * height)\n",
    "\n",
    "    # Calculate the top-left and bottom-right coordinates of the bounding box\n",
    "    x1 = x - box_width // 2\n",
    "    y1 = y - box_height // 2\n",
    "    x2 = x1 + box_width\n",
    "    y2 = y1 + box_height\n",
    "\n",
    "    # Draw the bounding box\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    cv2.putText(image, class_label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "# Display the image\n",
    "# plt.imshow(image)\n",
    "cv2.imshow('Annotated Image', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.112  Python-3.9.13 torch-2.0.1+cu118 CUDA:0 (NVIDIA GeForce RTX 3050 Ti Laptop GPU, 4096MiB)\n",
      "Setup complete  (12 CPUs, 15.4 GB RAM, 349.1/475.3 GB disk)\n"
     ]
    }
   ],
   "source": [
    "import ultralytics\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.0.120 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.112  Python-3.9.13 torch-2.0.1+cu118 CUDA:0 (NVIDIA GeForce RTX 3050 Ti Laptop GPU, 4096MiB)\n",
      "\u001b[34m\u001b[1myolo\\engine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=data.yaml, epochs=3, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=0, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs\\detect\\train3\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3011043 parameters, 3011027 gradients\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train3', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "WARNING  NMS time limit 0.550s exceeded\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\vnang\\Documents\\Lomba\\GEMASTIK 2023\\RDD Using YOLO\\data\\train\\labels... 17062 images, 0 backgrounds, 0 corrupt: 100%|██████████| 17062/17062 [00:46<00:00, 366.24it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\vnang\\Documents\\Lomba\\GEMASTIK 2023\\RDD Using YOLO\\data\\train\\images\\Japan_001008.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\vnang\\Documents\\Lomba\\GEMASTIK 2023\\RDD Using YOLO\\data\\train\\images\\Japan_001086.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\vnang\\Documents\\Lomba\\GEMASTIK 2023\\RDD Using YOLO\\data\\train\\images\\Japan_002908.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\vnang\\Documents\\Lomba\\GEMASTIK 2023\\RDD Using YOLO\\data\\train\\images\\Japan_002934.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\vnang\\Documents\\Lomba\\GEMASTIK 2023\\RDD Using YOLO\\data\\train\\images\\Japan_002964.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\vnang\\Documents\\Lomba\\GEMASTIK 2023\\RDD Using YOLO\\data\\train\\images\\Japan_003494.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\vnang\\Documents\\Lomba\\GEMASTIK 2023\\RDD Using YOLO\\data\\train\\images\\Japan_004318.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\vnang\\Documents\\Lomba\\GEMASTIK 2023\\RDD Using YOLO\\data\\train\\images\\Japan_004938.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\vnang\\Documents\\Lomba\\GEMASTIK 2023\\RDD Using YOLO\\data\\train\\images\\Japan_005685.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\vnang\\Documents\\Lomba\\GEMASTIK 2023\\RDD Using YOLO\\data\\train\\images\\Japan_006215.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\vnang\\Documents\\Lomba\\GEMASTIK 2023\\RDD Using YOLO\\data\\train\\images\\Japan_008673.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\vnang\\Documents\\Lomba\\GEMASTIK 2023\\RDD Using YOLO\\data\\train\\images\\Japan_011427.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\vnang\\Documents\\Lomba\\GEMASTIK 2023\\RDD Using YOLO\\data\\train\\images\\Japan_011986.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\vnang\\Documents\\Lomba\\GEMASTIK 2023\\RDD Using YOLO\\data\\train\\images\\Japan_012446.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\vnang\\Documents\\Lomba\\GEMASTIK 2023\\RDD Using YOLO\\data\\train\\images\\United_States_002943.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\vnang\\Documents\\Lomba\\GEMASTIK 2023\\RDD Using YOLO\\data\\train\\labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\vnang\\Documents\\Lomba\\GEMASTIK 2023\\RDD Using YOLO\\data\\val\\labels... 4266 images, 0 backgrounds, 0 corrupt: 100%|██████████| 4266/4266 [00:13<00:00, 319.46it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING  C:\\Users\\vnang\\Documents\\Lomba\\GEMASTIK 2023\\RDD Using YOLO\\data\\val\\images\\Japan_001921.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING  C:\\Users\\vnang\\Documents\\Lomba\\GEMASTIK 2023\\RDD Using YOLO\\data\\val\\images\\Japan_006916.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING  C:\\Users\\vnang\\Documents\\Lomba\\GEMASTIK 2023\\RDD Using YOLO\\data\\val\\images\\Japan_007271.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING  C:\\Users\\vnang\\Documents\\Lomba\\GEMASTIK 2023\\RDD Using YOLO\\data\\val\\images\\Japan_009457.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\vnang\\Documents\\Lomba\\GEMASTIK 2023\\RDD Using YOLO\\data\\val\\labels.cache\n",
      "Plotting labels to runs\\detect\\train3\\labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train3\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        1/3      2.35G      1.991      2.702      1.762         37        640: 100%|██████████| 1067/1067 [13:21<00:00,  1.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 134/134 [02:24<00:00,  1.08s/it]\n",
      "                   all       4266      10335      0.334      0.317      0.242      0.105\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        2/3      2.38G      1.894      2.216      1.642         20        640: 100%|██████████| 1067/1067 [16:39<00:00,  1.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 134/134 [02:01<00:00,  1.10it/s]\n",
      "                   all       4266      10335      0.401      0.341      0.296      0.132\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        3/3       2.4G      1.859        2.1      1.612         25        640: 100%|██████████| 1067/1067 [12:19<00:00,  1.44it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 134/134 [01:55<00:00,  1.16it/s]\n",
      "                   all       4266      10335       0.42      0.356      0.324      0.146\n",
      "\n",
      "3 epochs completed in 0.814 hours.\n",
      "Optimizer stripped from runs\\detect\\train3\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from runs\\detect\\train3\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating runs\\detect\\train3\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.0.112  Python-3.9.13 torch-2.0.1+cu118 CUDA:0 (NVIDIA GeForce RTX 3050 Ti Laptop GPU, 4096MiB)\n",
      "Model summary (fused): 168 layers, 3005843 parameters, 0 gradients\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 134/134 [01:48<00:00,  1.24it/s]\n",
      "                   all       4266      10335      0.421      0.356      0.324      0.146\n",
      "Speed: 0.4ms preprocess, 12.5ms inference, 0.0ms loss, 2.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('yolov8n.pt')\n",
    "model.train(data='data.yaml', save=True, epochs=3, imgsz=640, device=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
